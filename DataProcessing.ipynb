{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"DataProcessing-1.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TUrs5IcrywuV"},"source":["### 声明\n","\n","* 该代码源自：https://github.com/mhsamavatian/DAP/blob/master/1-GenerateFeatureVector\n","\n","* 由于我们下载的数据集有所更新、我们希望提取的特征也有所不同，所以我们对原始代码进行了一定的改动\n","\n","### 代码功能\n","\n","* 载入交通事件数据: https://smoosavi.org/datasets/lstw\n","\n","* 载入气象和光照数据：https://github.com/mhsamavatian/DAP/tree/master/data\n","\n","* 将研究区域栅格化为5km*5km的单元格，并给每个小单元格分配一个geohash\n","\n","* 把研究的时间段切分成以time_interval分钟为间隔的时间步\n","\n","* 对于每个geohash和每个时间步，初步生成其特征向量（包含时间、交通事件、气候）"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PiFfxRPLP9j","executionInfo":{"status":"ok","timestamp":1623481763562,"user_tz":-480,"elapsed":27025,"user":{"displayName":"Junlin Chen","photoUrl":"","userId":"04548097444605488042"}},"outputId":"237c6ecf-a105-4063-e5a3-0c3afefa0fe3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7GNe7RijLyR3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623482100928,"user_tz":-480,"elapsed":5762,"user":{"displayName":"Junlin Chen","photoUrl":"","userId":"04548097444605488042"}},"outputId":"9c6930ff-23b3-4ab5-efcf-387470479831"},"source":["!pip install pygeohash\n","!pip install haversine"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pygeohash in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: haversine in /usr/local/lib/python3.7/dist-packages (2.3.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JcfjQuSCywuZ"},"source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime,timedelta\n","import pytz\n","import pygeohash as gh\n","from haversine import haversine\n","import time\n","import json\n","import math\n","from sklearn.preprocessing import OneHotEncoder\n","\n","grid_width = 5 # 栅格化粒度（每个小单元格的边长）\n","time_interval = 15 # 时间间隔"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wO8zdvTRywub"},"source":["cities = {'Austin': [30.079327, 30.596764,-97.968881,-97.504838]}\n","\n","time_zones = {'Austin':'US/Central'}\n","\n","# 研究时间间隔\n","start = datetime(2018, 6, 1)\n","finish   = datetime(2018, 9, 2)\n","\n","begin = datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n","end   = datetime.strptime('2018-08-31 23:59:59', '%Y-%m-%d %H:%M:%S')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YI6kECXIywuc"},"source":["### 载入交通事件数据"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbP4htOyNlxv","executionInfo":{"status":"ok","timestamp":1623471791099,"user_tz":-480,"elapsed":149320,"user":{"displayName":"Junlin Chen","photoUrl":"","userId":"04548097444605488042"}},"outputId":"6624daff-f602-49ea-da47-3f210e0e3e3c"},"source":["! tar -zxvf /content/drive/MyDrive/Capstone/data/TrafficEvents_Aug16_Dec20_Publish.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TrafficEvents_Aug16_Dec20_Publish.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t7ywuOqJN2IK"},"source":["# 由于新数据集的数据量太大，我们需要分块读取\n","\n","chunksize=1000000\n","\n","traffic_event = pd.DataFrame()\n","\n","for temp_event in pd.read_csv('TrafficEvents_Aug16_Dec20_Publish.csv',chunksize=chunksize):\n","  temp_event['StartTime(UTC)'] = temp_event['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","  temp_event['EndTime(UTC)'] = temp_event['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","  if (min(temp_event['EndTime(UTC)']) <= end and max(temp_event['StartTime(UTC)']) >= start):\n","    traffic_event = pd.concat([traffic_event,temp_event], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q67tAn2zPnF5"},"source":["# weather_event = pd.read_csv(u'drive/MyDrive/Capstone/data/WeatherEvents_Aug16_Dec20_Publish.csv')\n","# weather_event.head()\n","\n","# weather_event['StartTime(UTC)'] = weather_event['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","# weather_event['EndTime(UTC)'] = weather_event['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"joIU4U9Jywud"},"source":["traffic_event['StartTime(UTC)'] = traffic_event['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","traffic_event['EndTime(UTC)'] = traffic_event['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","# weather_event['StartTime(UTC)'] = weather_event['StartTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')\n","# weather_event['EndTime(UTC)'] = weather_event['EndTime(UTC)'].astype('datetime64[ns]', errors = 'ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skRzOC9yywud","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623472242142,"user_tz":-480,"elapsed":2342,"user":{"displayName":"Junlin Chen","photoUrl":"","userId":"04548097444605488042"}},"outputId":"4405ee6d-671e-42cc-e1d2-111a5fcf481c"},"source":["for c in cities:\n","    crds = cities[c]\n","    subset_all = traffic_event[(traffic_event['StartTime(UTC)'] >= start) & (traffic_event['StartTime(UTC)'] < end) & \n","                    (traffic_event['LocationLat']>crds[0]) & (traffic_event['LocationLat']<crds[1]) & (traffic_event['LocationLng']>crds[2]) & \n","                    (traffic_event['LocationLng']<crds[3])]\n","    \n","    subset_accidents = traffic_event[(traffic_event['Type']=='Accident') & (traffic_event['StartTime(UTC)'] >= start) & (traffic_event['StartTime(UTC)'] < finish) \n","                          & (traffic_event['LocationLat']>crds[0]) & (traffic_event['LocationLat']<crds[1]) & (traffic_event['LocationLng']>crds[2]) \n","                          & (traffic_event['LocationLng']<crds[3])]\n","    \n","    print('For {} we have {} incidents, with {} accidents! ratio {:.2f}'.format(c, len(subset_all), len(subset_accidents), len(subset_accidents)*1.0/len(subset_all)))\n","    \n","    subset_all.to_csv(u'drive/MyDrive/Capstone/data/traffic_event_data.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For Austin we have 20435 incidents, with 4413 accidents! ratio 0.22\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XUzXJQHS5jml"},"source":["# code_number = {}\n","\n","# for c in cities:\n","#     with open(path + 'MQ_{}_20180601_20180609.csv'.format(c), 'r') as file:\n","#         header = False\n","#         for line in file:\n","#             if not header:\n","#                 header = True\n","#                 continue\n","#             parts = line.replace('\\r', '').replace('\\n', '').split(',')\n","#             temp =  code_conversion[code_conversion.Code==float(parts[3])].C.values\n","#             if (len(temp)>0):\n","#               if int(temp[0]) in code_number:\n","#                 code_number[int(temp[0])] += 1\n","#               else:\n","#                 code_number[int(temp[0])] = 1\n","\n","code_conversion = pd.read_csv(u'drive/MyDrive/Capstone/data/event_code.csv')\n","\n","name_conversion = {}\n","\n","for c in cities:\n","    with open(u'drive/MyDrive/Capstone/data/traffic_event_data.csv', 'r') as file:\n","        header = False\n","        for line in file:\n","            if not header:\n","                header = True\n","                continue\n","            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n","            code = parts[3]\n","            temp = code_conversion[code_conversion.Code==int(code)].C.values\n","            if (len(temp)>0):\n","              C = temp[0]\n","              if (C == 1 or C == 20):\n","                name_conversion[code]='Congestion'\n","              elif (C == 3):\n","                name_conversion[code]='Accident'\n","              elif (C == 4):\n","                name_conversion[code]='Incident'\n","              elif (C == 5 or C == 7 or C == 8 or C == 9):\n","                name_conversion[code]='Restriction'\n","              elif (C == 11 or C == 12 or C == 13):\n","                name_conversion[code]='Obstruction'\n","              elif (C == 18):\n","                name_conversion[code]='Activity'\n","              elif (C == 25):\n","                name_conversion[code]='Equipment'\n","              else:\n","                name_conversion[code]='Other'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YrrF2QiZywuh"},"source":["### 将交通事件数据匹配至每个单元格内"]},{"cell_type":"code","metadata":{"id":"U2Ycdioyywui"},"source":["zone_to_be = {}\n","\n","for z in ['US/Central']:\n","    t_begin = begin.replace(tzinfo=pytz.timezone(z))\n","    t_end   = end.replace(tzinfo=pytz.timezone(z))\n","    zone_to_be[z] = [t_begin, t_end]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCL6H9KIywui"},"source":["def return_interval_index(time_stamp, start, end):\n","    if time_stamp < start or time_stamp > end: \n","        return -1\n","    index = int(((time_stamp-start).days*24*60 + (time_stamp-start).seconds/60)/time_interval)\n","    return index\n","\n","diff = math.ceil(((end-begin).days*24*60 + (end-begin).seconds/60)/time_interval)\n","\n","city_to_geohashes = {}\n","for c in cities: city_to_geohashes[c] = {}\n","\n","start_timestamp = time.time()\n","\n","geocode_to_airport = {}\n","aiport_to_timezone = {}\n","\n","for c in cities:\n","    z = time_zones[c]\n","    \n","    with open(u'drive/MyDrive/Capstone/data/traffic_event_data.csv', 'r') as file:\n","        header = False\n","        for line in file:\n","            if not header:\n","                header = True\n","                continue\n","            parts = line.replace('\\r', '').replace('\\n', '').split(',')\n","            \n","            ds = datetime.strptime(parts[5].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n","            ds = ds.replace(tzinfo=pytz.utc)\n","            ds = ds.astimezone(pytz.timezone(z))\n","            s_interval = return_interval_index(ds, zone_to_be[z][0], zone_to_be[z][1])\n","            if s_interval==-1: continue\n","                \n","            de = datetime.strptime(parts[6].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n","            de = de.replace(tzinfo=pytz.utc)\n","            de = de.astimezone(pytz.timezone(z))\n","            e_interval = return_interval_index(de, zone_to_be[z][0], zone_to_be[z][1])\n","            if e_interval == -1: e_interval = diff-1    \n","            \n","            start_gh = gh.encode(float(parts[8]), float(parts[9]), precision=grid_width) #事件所在的geohash\n","            \n","            intervals = []\n","            if start_gh not in city_to_geohashes[c]:\n","                for i in range(diff):\n","                    intervals.append({'Congestion':0, 'Accident':0, 'Incident':0, 'Restriction':0,\n","                                      'Obstruction':0, 'Activity':0, 'Equipment':0, 'Other':0})\n","            else:\n","                intervals = city_to_geohashes[c][start_gh]\n","            \n","            if parts[3] in name_conversion:\n","                tp = name_conversion[parts[3]]\n","                \n","            for i in range(s_interval, e_interval+1):                \n","                v = intervals[i]\n","                if tp in v: v[tp] = v[tp] + 1\n","                else: v['Other'] = v['Other'] + 1\n","                intervals[i] = v\n","                \n","                if tp == 'Accident': break # unlike other types of traffic events\n","                \n","            city_to_geohashes[c][start_gh] = intervals #更新事件\n","\n","            ap = parts[11]\n","            if len(ap) > 3:\n","                if start_gh not in geocode_to_airport:\n","                    geocode_to_airport[start_gh] = set([ap])\n","                else:\n","                    st = geocode_to_airport[start_gh]\n","                    st.add(ap)\n","                    geocode_to_airport[start_gh] = st\n","                aiport_to_timezone[ap] = z\n","  \n","    \n","    print ('Done with {} in {:.1f} sec! there are {} geohashes with data!'.format(c, \n","                                time.time()-start_timestamp, len(city_to_geohashes[c])))\n","    start_timestamp = time.time()   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdwJjnOEywuk"},"source":["### 载入天气数据"]},{"cell_type":"code","metadata":{"id":"r8dFzTFPywua"},"source":["class weather:\n","    date = ''\n","    temp = 0.0\n","    windchill = 0.0\n","    humid = 0.0\n","    pressure= 0.0\n","    visib = 0.0\n","    windspeed = 0.0\n","    winddir = ''\n","    precipitation = 0.0\n","    events = ''\n","    condition = ''\n","    \n","    def __init__(self, date, temp, windchill, humid, pressure, visib, windspeed, winddir, \n","                 precipitation, events, condition, zone):\n","        self.date = datetime.strptime(date, '%Y-%m-%d %I:%M:%S %p')\n","        self.date = self.date.replace(tzinfo=pytz.timezone(zone))\n","        self.temp = float(temp)\n","        self.windchill = float(windchill)\n","        self.humid = float(humid)\n","        self.pressure = float(pressure)\n","        self.visib = float(visib)\n","        self.windspeed = float(windspeed)\n","        self.winddir = winddir\n","        self.precipitation = float(precipitation)\n","        self.events = events\n","        self.condition = condition"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBiXs8d2ywuk"},"source":["# load and sort relevant weather data\n","airports_to_observations = {}\n","for g in geocode_to_airport:\n","    aps = geocode_to_airport[g]\n","    for a in aps:\n","        if a not in airports_to_observations:\n","            airports_to_observations[a] = []\n","\n","print ('{} airports to collect data for!'.format(len(airports_to_observations)))\n","            \n","w_path = u'drive/MyDrive/Capstone/data/weather_data/' # this directory contains weather observation records for each airport\n","airport_to_data = {}\n","for ap in airports_to_observations:\n","    data = []\n","    z = aiport_to_timezone[ap]\n","    print('Airport {}'.format(ap))\n","    header = ''\n","    try:\n","        with open(w_path + ap + '.csv', 'r') as file:\n","            for line in file:\n","                if 'Airport' in line: \n","                    header = line.replace('\\r','').replace('\\n','').replace(',Hour','')\n","                    continue\n","                parts = line.replace('\\r', '').replace('\\n', '').split(',')\n","                try:\n","                    w = weather(parts[1] + ' ' + parts[2].split(' ')[0] + ':00 ' + parts[2].split(' ')[1], parts[3], parts[4], \n","                              parts[5], parts[6], parts[7], parts[8], parts[9], parts[10], parts[11], parts[12], z)   \n","                    data.append(w)\n","                except:\n","                    continue\n","\n","            data.sort(key=lambda x:x.date)\n","            airport_to_data[ap] = data\n","    except:\n","        print('Error')\n","        #把有关这个机场的索引都删除\n","        del_geocodes = []\n","        for g in geocode_to_airport:\n","          aps = geocode_to_airport[g]\n","          if ap in aps:\n","            aps.remove(ap)\n","          geocode_to_airport[g] = aps\n","          if (len(aps)==0):\n","            del_geocodes.append(g)\n","        \n","        for key in del_geocodes:\n","          del geocode_to_airport[key]\n","        \n","        continue\n","    \n","print ('\\nData for {} airport stations is loaded!'.format(len(airport_to_data)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1v7280Roywul"},"source":["for c in city_to_geohashes:\n","    for g in city_to_geohashes[c]:\n","        if g not in geocode_to_airport:\n","            print('Found')\n","            gc = gh.decode_exactly(g)[0:2]\n","            min_dist = 1000000000\n","            close_g = ''\n","            for _g in geocode_to_airport:\n","                _gc = gh.decode_exactly(_g)[0:2]\n","                dst = haversine(gc, _gc, 'km')\n","                if dst < min_dist:\n","                    min_dist = dst\n","                    close_g = _g\n","#             print g, close_g, min_dist\n","            geocode_to_airport[g] = geocode_to_airport[close_g]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IcAU25Tywul"},"source":["city_to_geohashes_to_weather = {}\n","\n","for c in city_to_geohashes: #不同城市\n","    start = time.time()\n","    geo2weather = {}\n","    for g in city_to_geohashes[c]: #不同栅格\n","        w_data = []\n","        for i in range(len(city_to_geohashes[c][g])):\n","            w_data.append({'Temperature':[], 'Humidity':[], 'Pressure':[], 'Visibility':[], 'WindSpeed':[], \n","                          'Precipitation':[], 'Condition':set(), 'Event':set()})\n","        # populate weather data\n","        aps = geocode_to_airport[g]\n","        for a in aps: #不同机场\n","            z = aiport_to_timezone[a]\n","            a_w_data = airport_to_data[a] #天气数据\n","            prev = 0\n","            for a_w_d in a_w_data: #不同时刻的天气\n","                idx = return_interval_index(a_w_d.date, zone_to_be[z][0], zone_to_be[z][1]) #该时刻对应的tid\n","                if idx >-1:\n","                    for i in range(prev, min(idx+1, len(w_data))):\n","                        _w = w_data[i]\n","                        \n","                        _tmp = _w['Temperature']\n","                        if a_w_d.temp > -1000:\n","                            _tmp.append(a_w_d.temp)\n","                            _w['Temperature'] = _tmp\n","                        \n","                        _hmd = _w['Humidity']\n","                        if a_w_d.humid > -1000:\n","                            _hmd.append(a_w_d.humid)\n","                            _w['Humidity'] = _hmd\n","                        \n","                        _prs = _w['Pressure']\n","                        if a_w_d.pressure > -1000:\n","                            _prs.append(a_w_d.pressure)\n","                            _w['Pressure'] = _prs\n","                        \n","                        _vis = _w['Visibility']\n","                        if a_w_d.visib > -1000:\n","                            _vis.append(a_w_d.visib)\n","                            _w['Visibility'] = _vis\n","                            \n","                        _wspd = _w['WindSpeed']\n","                        if a_w_d.windspeed > -1000:\n","                            _wspd.append(a_w_d.windspeed)\n","                            _w['WindSpeed'] = _wspd\n","                            \n","                        _precip = _w['Precipitation']\n","                        if a_w_d.precipitation > -1000:\n","                            _precip.append(a_w_d.precipitation)\n","                            _w['Precipitation'] = _precip\n","                            \n","                        _cond = _w['Condition']\n","                        _cond.add(a_w_d.condition)\n","                        _w['Condition'] = _cond\n","                        \n","                        _evnt = _w['Event']\n","                        _evnt.add(a_w_d.events)\n","                        _w['Event'] = _evnt\n","                        \n","                        w_data[i] = _w\n","                        \n","                    prev = idx+1\n","                                                \n","        geo2weather[g] = w_data\n","    city_to_geohashes_to_weather[c] = geo2weather\n","    print('Done with {} in {:.1f} sec!'.format(c, time.time()-start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FCOK5So3ywum"},"source":["### 载入光照数据"]},{"cell_type":"code","metadata":{"id":"8zR19_EUywum"},"source":["class dayLight:\n","    sunrise = []\n","    sunset = []\n","    def __init__(self, sunrise, sunset):\n","        self.sunrise = sunrise\n","        self.sunset = sunset\n","        \n","def return_time(x):\n","    try:\n","        h = int(x.split(':')[0])\n","        m = int(x.split(':')[1].split(' ')[0])\n","        if 'pm' in x and h < 12: h = h + 12\n","        return [h,m]\n","    except: return [0,0]\n","\n","    \n","def returnDayLight(city, state, dt):\n","    sc = city + '-' + state\n","    days = city_days_time[sc]\n","    d = str(dt.year) + '-' + str(dt.month) + '-' + str(dt.day)\n","    if d in days:\n","        r = days[d]\n","        if ((dt.hour>r.sunrise[0] and dt.hour<r.sunset[0]) or\n","            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<r.sunset[0]) or\n","            (dt.hour>r.sunrise[0] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1]) or \n","            (dt.hour>=r.sunrise[0] and dt.minute>=r.sunrise[1] and dt.hour<=r.sunset[0] and dt.minute<r.sunset[1])):\n","            return '1'\n","        else: return '0'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRkhgo_Pywum"},"source":["city_days_time = {}\n","\n","days = {}\n","city = ''\n","with open(u'drive/My Drive/Capstone/data/daylight_data.csv', 'r') as file: # you find daylight data for the selected 6 cities in this file\n","    for ln in file.readlines():\n","        parts = ln.replace('\\r','').replace('\\n','').split(',')\n","\n","        if parts[0] != city:\n","            if len(city) > 0: \n","                if city in city_days_time:\n","                    _days = city_days_time[city]\n","                    for _d in _days: days[_d] = _days[_d]\n","                city_days_time[city] = days\n","\n","            city = parts[0]\n","            days = {}\n","\n","        sunrise = return_time(parts[2])\n","        sunset  = return_time(parts[3])\n","        dl = dayLight(sunrise, sunset)\n","        days[parts[1]] = dl\n","\n","if city in city_days_time:\n","    _days = city_days_time[city]\n","    for _d in _days: days[_d] = _days[_d]\n","city_days_time[city] = days\n","\n","\n","print('Successfully loaded daylight data for {} cities!'.format(len(city_days_time)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8BMOendywun"},"source":["# pre-load daylight mapping for different cities\n","city_to_index_to_daylight = {}\n","states = {'Austin':'TX'}\n","for c in cities:\n","    d_begin = begin.replace(tzinfo=pytz.timezone(time_zones[c]))\n","    d_end   = end.replace(tzinfo=pytz.timezone(time_zones[c]))\n","    index_to_daylight = {}\n","    index = 0\n","    while(d_begin < d_end):\n","        dl = returnDayLight(c, states[c], d_begin)\n","        index_to_daylight[index] = dl\n","        index += 1\n","        d_begin += timedelta(seconds=time_interval*60)\n","    city_to_index_to_daylight[c] = index_to_daylight"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTDzfm4fywun"},"source":["### 数据整合"]},{"cell_type":"code","metadata":{"id":"Bua5HVDTywun"},"source":["# map each time-step to hour of day and day of the week; this should be consistent across different time-zones!\n","timestep_to_dow_hod = {}\n","d_begin = begin.replace(tzinfo=pytz.utc)\n","d_end   = end.replace(tzinfo=pytz.utc)\n","index = 0\n","\n","while (d_begin < d_end):\n","    dow = d_begin.weekday()\n","    hod = d_begin.hour\n","    timestep_to_dow_hod[index] = [dow, hod]\n","    d_begin += timedelta(seconds=time_interval*60)    \n","    index += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"z4HBkrW6ywun"},"source":["traffic_tags =  ['Accident', 'Congestion', 'Incident', 'Restriction', 'Obstruction', 'Activity', 'Equipment', 'Other']\n","weather_tags = ['Condition', 'Event', 'Humidity', 'Precipitation', 'Pressure', 'Temperature', 'Visibility', 'WindSpeed']\n","poi_tags = []\n","start = time.time()\n","condition_tags = set()\n","\n","for c in city_to_geohashes:\n","    # creating vector for each reion (geohash) during a time_interval minutes time interval. Such vector contains time, traffic, and weather attributes. \n","    writer = open(u'temp_data_{}_{}.csv'.format(str(grid_width),str(time_interval)), 'w')\n","    writer.write('Geohash,TimeStep,DOW,HOD,DayLight,T-Accident,T-Congestion,T-Incident,T-Restriction,'\\\n","        'T-Obstruction,T-Activity,T-Equipment,T-Other,W-Humidity,W-Precipitation,W-Pressure,'\\\n","        'W-Temperature,W-Visibility,W-WindSpeed,W-Rain,W-Snow,W-Fog,W-Hail\\n')\n","    \n","    traffic = city_to_geohashes[c]\n","    weather = city_to_geohashes_to_weather[c]        \n","    for g in traffic:\n","        vectors = []\n","        for i in range(len(traffic[g])):\n","            v = []\n","            for t in traffic_tags: v.append(traffic[g][i][t])\n","            v_w = [0,0,0,0] # for rain, snow, fog, and hail\n","            for w in weather_tags:\n","                if w=='Condition' or w=='Event':      \n","                    _tgs = weather[g][i][w]\n","                    for _tg in _tgs: \n","                        if 'rain' in _tg.lower() or 'drizzle' in _tg.lower() or 'thunderstorm' in _tg.lower(): v_w[0] = 1\n","                        elif 'snow' in _tg.lower(): v_w[1] = 1\n","                        elif 'fog' in _tg.lower() or 'haze' in _tg.lower() or 'mist' in _tg.lower() or 'smoke' in _tg.lower(): v_w[2] = 1\n","                        elif 'hail' in _tg.lower() or 'ice pellets' in _tg.lower(): v_w[3] = 1                            \n","                elif len(weather[g][i][w]) == 0: v.append(0)\n","                else: v.append(np.mean(weather[g][i][w]))\n","            for _v_w in v_w: v.append(_v_w)\n","            vectors.append(v)\n","        \n","        for i in range(len(vectors)):\n","            v = vectors[i]\n","            v = [str(v[j]) for j in range(len(v))]\n","            v = ','.join(v)\n","            writer.write(g + ',' + str(i) + ',' + str(timestep_to_dow_hod[i][0]) + ',' + str(timestep_to_dow_hod[i][1]) \n","                         + ',' + city_to_index_to_daylight[c][i] + ',' + v + '\\n')\n","            \n","    writer.close()\n","    print ('Done with {} in {:.1f} sec! #vectors {}!'.format(c, time.time()-start, len(traffic)*len(vectors)))\n","    start = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LRMdcH9K3xP"},"source":["import pickle\n","\n","geohash_map = pd.read_csv(u'drive/MyDrive/Capstone/data/poi_data.csv')\n","geo_dict = dict(zip(geohash_map.Geohash.unique(), range(len(geohash_map.Geohash.unique()))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-t10rtLRkj0"},"source":["df = pd.read_csv(u'temp_data_{}_{}.csv'.format(str(grid_width),str(time_interval)))\n","\n","print (\"zero accident =\",float(df[df['T-Accident']==0].shape[0])/df.shape[0])\n","\n","def fun_hash(geohash):\n","    return geo_dict[geohash]\n","\n","df['geohash_code'] = df.apply(lambda row: fun_hash(row['Geohash']), axis=1) \n","\n","df = df [[u'TimeStep', u'T-Accident',u'Geohash',u'geohash_code', u'HOD', u'DOW', u'DayLight',\n","    u'T-Congestion', u'T-Incident', u'T-Restriction', u'T-Obstruction',\n","    u'T-Activity', u'T-Equipment', u'T-Other', u'W-Humidity',\n","    u'W-Precipitation', u'W-Pressure', u'W-Temperature', u'W-Visibility',\n","    u'W-WindSpeed', u'W-Rain', u'W-Snow', u'W-Fog', u'W-Hail']]\n","\n","def week_day(DOW):\n","    if DOW < 5:\n","        return 1\n","    else:\n","        return 0\n","def shift(group):\n","    df_list=[]\n","    for idx,df in group:\n","        df['predicted_accident'] = df['T-Accident'].shift(-1)\n","        df.drop(df.tail(1).index,inplace=True)\n","        df_list.append(df)\n","    return pd.concat(df_list)\n","\n","def time_category(HOD):\n","    if HOD >=6 and HOD <10:\n","        return 0\n","    if HOD >= 10 and HOD<14:\n","        return 1\n","    if HOD >=14 and HOD< 18:\n","        return 2;\n","    if HOD >=18 and HOD< 22:\n","        return 3\n","    else:\n","        return 4; \n","        \n","def make_binary(d):\n","    if d > 0:\n","        return 1\n","    else:\n","        return 0\n","\n","def onhot_enoceder(df):\n","     myEncoder = OneHotEncoder(sparse=False)\n","     myEncoder.fit(df['HOD_cat'].values.reshape(-1, 1))\n","\n","     onehot_encode = pd.concat([df.reset_index().drop('HOD_cat',1),\n","                 pd.DataFrame(myEncoder.transform(df['HOD_cat'].values.reshape(-1, 1)),\n","                              columns=['HOD_en0','HOD_en1','HOD_en2','HOD_en3','HOD_en4'])], axis=1).reindex()\n","\n","     return onehot_encode.drop('index',1)\n","\n","df['DOW_cat'] = df.apply(lambda row: week_day(row['DOW']), axis=1)   \n","df['HOD_cat'] = df.apply(lambda row: time_category(row['HOD']), axis=1) \n","df['T-Accident'] = df.apply(lambda row: make_binary(row['T-Accident']), axis=1) \n","group = df.groupby('Geohash')\n","df = shift(group)\n","df = onhot_enoceder(df)\n","\n","df = df [[u'TimeStep', u'predicted_accident',u'Geohash',u'geohash_code', u'DOW_cat', u'DayLight',\n","    u'HOD_en0', u'HOD_en1', u'HOD_en2', u'HOD_en3', u'HOD_en4', u'T-Accident',\n","    u'T-Congestion', u'T-Incident', u'T-Restriction', u'T-Obstruction',\n","    u'T-Activity', u'T-Equipment', u'T-Other', u'W-Humidity',\n","    u'W-Precipitation', u'W-Pressure', u'W-Temperature', u'W-Visibility',\n","    u'W-WindSpeed', u'W-Rain', u'W-Snow', u'W-Fog', u'W-Hail']]\n","\n","display (df.head())\n","\n","df.to_csv(u'drive/MyDrive/Capstone/data/integrated_data_{}_{}.csv'.format(str(grid_width),str(time_interval)))"],"execution_count":null,"outputs":[]}]}